{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import jieba\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\TIANJI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.656 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['美军 飞机 错误 地 把 中国 援助 日本 物资 偷运 到 美国 日本 日前 在 调运 中国 捐赠 的 2000 个 检测 试剂盒 期间 用 的 是 驻 日美军 的 运输机 结果 到达 目的地 之后 才 发现 这批 检测 试剂盒 已经 被 运到 美国 本土 美国共和党 议员 竟然 声称 检测 试剂盒 已经 用 完 了 现在 讨论 这个 问题 毫无意义 美国国会 大厦 参众两院 大楼 关闭', '襄阳 正式 封城 至此 整个 湖北 封省 封 一个 省护 一 国人 大家 多多保重 2 襄阳', '咱们 雷 神山 牛 防疫 有 效果 也 别 为 美国 瞎操心 了 美国 的 10 艘 医疗 军舰 已经 开进 纽约港 每艘 船上 有 1000 张 病床 上面 拥有 所以 的 医疗 抢救 设施 每艘 船 相当于 一个 雷 神山 明显 美国 对 远程 战争 早有 准备 没想到 第三次 世界大战 就 这样 开始 了 是 人类 对 病毒 的 战争 全国 确诊 新型 肺炎 病例 疫情 依旧 言殇', '武钢 武钢 董事长 邓琦琳 不是 应该 在 坐牢 吗 怎么 在 香港 到处 传播 病毒 确诊 肺炎 这是 谣言 还是 被 肺炎 抓住 的 贪官 2 青岛', '我 不是 中国 人 华人 女子 获 美国 绿卡 慷慨 对美 捐赠 20 万 只 口罩 我 已经 获得 美国 绿卡 即将 加入 美国 国籍 现在 已经 不是 中国 人 短短的 一句 话 在 海外 社交 网站 引起 了 巨大 轰动 据 海外 媒体 整理 这段话 是 一名 来自 中国宁波 的 女子 为了 快速 加入 美国 国籍 该 女子 以 个人 名义 在 中国 采购 了 20 万 只 口罩 捐赠 给 美国 国土 安全部 抵抗 疫情 为了 获得 美国 绿卡 加入 美国 国籍 这名 女子 除了 捐赠 口罩 之外 还 从 日本 采购 了 3 台 医疗 设备 献给 了 美国陆军 野战医院 用于 疑似 感染 的 美军 士兵 治疗 保守 估计 该 女子 现如今 已经 给 美国 捐赠 了 超过 300 万美元 的 物资 而 这 一切 仅仅 是 为了 加速 获得 美国 国籍 来源 兵戎 要志', '西安 爆料 逃离 武汉 大 作战 老 户县 新 鄠 hu 邑 yi 蒋村 小伙 吴 建章 此人 在 武汉 感染 后 逃离 回家 被 发现 后 在 户县 北关 医院 隔离 治疗 晚上 8 点 左右 又 一次 逃离 大家 年 都 过 不好 了 一只 老鼠 坏 了 一 锅汤 一声 叹息 一地 鸡毛 产经 智库 呼吁 希望 大家 转发 看到 此人 迅速 报警 别 让 病毒 再次 扩散', '最 硬 管控 网友 爆料 湖北 武穴市 凡 不 属于 疫情 防控 人员 上街 被 抓 的 一律 送到 市 体育馆 会展中心 学习 还要 完成 初三 黄冈 密卷 一张 成绩及格 者 可以 回家 怕 是 有人 14 天 也 回 不了 家 了 感受 到 了 来自 学校 的 关爱', '武汉 三民 小区 的 人 全变 红码 全市 所有人 全部 核酸 检测 武汉 三民 小区 出 了 六个 感染者 清零 很 久 以后 出 的 震动 全国 是 一个 89 岁 的 之前 感染 自己 好了没 去 治疗 隔 了 两个 月 又 发病 传染 人 武汉 发了 狠 所有人 除 六岁 以下 全部 强制 检测 三民 小区 封掉 5000 人 都 变红 码动 不了 又 隔离 14 天 上班 的 也 不行 了 全市 10 天都测 一遍 不测 的 人变 黄码 无法 出行 过期 检测 自己 出钱 要测 一个 小区 就 提前 封闭 这 是 搞 毛 了 被 病毒 害 的 太 惨大 样本 检测 科研 意义 也 很大', '张文宏 医生 自爆 收入 和 全家福 69 年 生人 浙江 瑞安 人 从小 喜欢 读书 学习 成绩优秀 在 老家 被 邻里 称为 神童 高考 考入 上海医学院 8 年 硕博 连读 后 留学 现是 全球 知名 的 传染病 专家 自爆 年收入 184 万 来源于 3 个 方面 1 华山医院 呼吸科 主任 工资 奖金 年入 50 万 2 国家 重点项目 学科 带头人 年入 120 万 3 每年 在 全球 顶尖 医学杂志 柳叶刀 上 发表 一篇 署名 论文 专利费 2 万美元 首个 公布 财产 的 医生 一个 干净 的 人强 在 看看 院士 高福 号称 在 柳叶刀 上 发表 过 500 多篇 论文 是不是 也 应该 公布 一下 自己 的 收入 呢 张文宏 大 浙江 2 北京 北京 天坛 饭店', '美国 的 10 艘 医疗 军舰 已经 开进 纽约港 每艘 船上 有 1000 张 病床 上面 拥有 所有 的 医疗 抢救 设施 每艘 船 相当于 一个 雷 神山 医院 如果 不是 这次 病毒 我们 都 不 知道 美国 的 医疗 底牌 呢 海外 疫情 特朗普 称 美国 经济 可能 走向 衰退', '我 不是 中国 人 华人 女子 获 美国 绿卡 慷慨 对美 捐赠 20 万 只 口罩 我 已经 获得 美国 绿卡 即将 加入 美国 国籍 现在 已经 不是 中国 人 短短的 一句 话 在 海外 社交 网站 引起 了 巨大 轰动 据 海外 媒体 整理 这段话 是 一名 来自 中国宁波 的 女子 为了 快速 加入 美国 国籍 该 女子 以 个人 名义 在 中国 采购 了 20 万 只 口罩 捐赠 给 美国 国土 安全部 抵抗 疫情 为了 获得 美国 绿卡 加入 美国 国籍 这名 女子 除了 捐赠 口罩 之外 还 从 日本 采购 了 3 台 医疗 设备 献给 了 美国陆军 野战医院 用于 疑似 感染 的 美军 士兵 治疗 保守 估计 该 女子 现如今 已经 给 美国 捐赠 了 超过 300 万美元 的 物资 而 这 一切 仅仅 是 为了 加速 获得 美国 国籍 来源 兵戎 要志', '我 不是 中国 人 华人 女子 获 美国 绿卡 慷慨 对美 捐赠 20 万 只 口罩 此 视频 已有 11 万次 播放 快 来 一起 看看 O 网页 链接', '祸不单行 美国 确诊 123 万 刚刚 凌晨 化工厂 大 爆炸 近 6 万人 受灾 上帝 也 要 惩罚 美利坚 了 吗 今日 热 搜 头条 美国 国内 疫情 严重 够 特朗普 喝 一壶 的 还有 心思 到处 惹是生非 甩锅 侮辱 中国 在 中东 波斯湾 搞 事情 威胁 伊朗 粗暴 干扰 他 国内 政人 在 做 天 在 看 美国 得克萨斯州 凌晨 一点 一座 化工厂 发生 严重 爆炸 火光 四起 巨大 火光 震动 波及 到 数公里 之外 的 居民 到 目前为止 已知 10 人 被 玻璃 划伤 数名 工人 伤亡 只 隔 几小时 又 发生 第二 波 爆炸 引起 惊恐万分 54 万 居民 连夜 紧急 撤退 到 现在 为止 究竟 是 天灾 还是 人祸 不得而知 但是 爆炸 释放出来 的 有毒气体 已经 袭击 无数 市民 似乎 是 上帝 在 提醒 美国 不要 搞 事情 挑起 战乱 制造事端 先 管理 好 自己 千疮百孔 的 国家 吐槽', '香港 失业率 52 创 十年 新高 香港特区政府 统计 处 19 日 公布 2 月 至 4 月经 季节性 调整 的 失业率 升至 52 创 逾 10 年 新高 就业 不足 率为 31 是 超过 15 年 来 的 高位 特区政府 劳工 及 福利 局局长 罗致 光 表示 新冠 疫情 继续 广泛 影响 经济 活动 劳工市场 急剧 恶化 2 月 至 4 月 的 总 就业人数 和 劳动 人口 均 创 历史 最大 按年 跌幅 零售 住宿 及 餐饮 服务 等 相关 行业 的 合计 失业率 急 升至 9 创 逾 15 年 新高 就业 不足 率 也 急 升至 59 是 有 纪录 以来 的 最高 位 人民日报 记者 陈然', '美国 新冠 肺炎 超 221 万 美国 日 新增 确诊 超 3 万例 据 美国 约翰斯 霍普金斯大学 疫情 实时 监测 系统 显示 截至 美 东 时间 6 月 19 日 下午 5 时 33 分 美国 已有 新冠 病毒感染 病例 2215587 例 其中 包括 死亡 病例 118991 例 与 该 系统 约 24 小时 前 的 数据 相比 美国 新增 感染 病例 33302 例 新增 死亡 病例 695 例 人民日报 记者 郑琪', '钟南山 称 不 从 全球 范围 内 控制 好 不 可能 战胜 疫情 3 月 12 日 广东省 人民政府 新闻 办公室 举行 新闻 发布会 钟南山 称 6 月份 结束 疫情 是 可以 期待 的 但是 也 要 取决于 各个 国家 的 重视 程度 要 在 两个 月 内 研发 出 特效药 或者 特殊 的 做法 还是 不太可能 因此 现在 一方面 要 加强 自身 管控 另一方面 要 加强 与 国外 的 交流 现在 不 从 全球 范围 内 控制 好 的话 是 不 可能 战胜 疫情', '日本 全国 紧急状态 将 延长 据 日本广播协会 NHK 电视台 统计 截至 4 日 10 时 30 分 北京 时间 9 时 30 分 日本 24 小时 内 新增 新冠 确诊 病例 202 例 累计 确诊 15079 例 新增 死亡 病例 19 例 累计 死亡 病例 536 例 日本首相 安倍晋三 将 于 4 日 傍晚 正式 宣布 延长 紧急状态 4 日 上午 日本政府 召集 了 专家 小组 举行 会议 讨论 疫情 长期化 状态 下 的 防疫 和 经济 活动 对策 之后 又 召开 了 咨询 委员会 会议 围绕 将 全国 紧急状态 延长 到 5 月 31 日 的 方针 向 咨询 委员会 专家 进行 咨询 新华社', '千里 为 邻战疫 必胜 湖北 捐助 黑龙江 首批 医用 物资 启程 赴 绥芬河 15 日 1106 湖北 向 黑龙江 捐助 的 第一批 医用 物资 从 武汉 天河 国际 机场 启程 飞往 黑龙江省 绥芬河市 这批 物资 包括 医用 防护服 5 万套 N95 口罩 10 万 只 医用 外科 口罩 100 万 只 一次性 医用 口罩 200 万 只 移动 DR1 台 正压 呼吸器 50 套及 配件 2500 套 空气 消毒机 300 套 有 创 呼吸机 5 套 心电监护 仪 100 台 注射 泵 121 台 输液泵 63 台 据介绍 自 1 月 27 日起 黑龙江省 共 派出 8 批 共计 1554 名 医护 工作者 援助 武汉市 和 孝感市 黑龙江 省委 省政府 还 紧急 调集 3000 吨 大米 等 物资 有力 保障 了 湖北 疫情 防控 物资 需求 王 翔宇 李伟张 小舟', '继续 加油 北京 连续 4 天零 新增 北京 中 风险 地区 15 个 7 月 9 日 0 时至 24 时 北京 无 新增 报告 本地 确诊 病例 疑似病例 和 无症状 感染者 治愈 出院 病例 12 例无 新增 报告 境外 输入 确诊 病例 疑似病例 和 无症状 感染者 6 月 11 日 0 时至 7 月 9 日 24 时 累计 报告 本地 确诊 病例 335 例在院 263 例 治愈 出院 72 例尚 在 观察 的 无症状 感染者 24 例无 新增 报告 境外 输入 新冠 肺炎 确诊 病例 疑似病例 和 无症状 感染者 7 月 8 日 0 时至 24 时 丰台区 南苑 街道 南苑 地区 乡 大兴区 观音寺 街道 由 中 风险 地区 调整 为 低 风险 地区 截至 7 月 8 日 24 时 我市 共有 高风险 地区 1 个 为 丰台区 花乡 地区 乡 共有 中 风险 地区 15 个 为 海淀区 田村路 街道 四季青 地区 镇 丰台区 丰台 街道 卢沟桥 街道 马家堡 街道 卢沟桥 地区 乡 新村 街道 大兴区 北 臧 村镇 黄村 地区 镇 青云 店镇 魏善 庄镇 兴丰 街道 高 米店 街道 西红门 地区 镇 昌平区 回龙观 街道', '习近平 同 美国 总统 特朗普 通电话 国家 主席 习近平 27 日 应约 同 美国 总统 特朗普 通电话 习近平 强调 新冠 肺炎 疫情 发生 以来 中方 始终 本着 公开 透明 负责 任 态度 及时 向世卫 组织 以及 包括 美国 在内 的 有关 国家 通报 疫情 信息 包括 第一 时间 发布 病毒基因 序列 等 信息 毫无保留 地同 各方 分享 防控 和 治疗 经验 并 尽己 所能 为 有 需要 的 国家 提供 支持 和 援助 我们 将 继续 这样 做同 国际 社会 一道 战胜 这场 疫情 习近平 指出 流行性 疾病 不 分 国界 和 种族 是 人类 共同 的 敌人 国际 社会 只有 共同 应对 才能 战而胜 之 在 各方 共同努力 下 昨天 举行 的 二 十国集团 领导人 应对 新冠 肺炎 特别 峰会 达成 不少 共识 取得 积极 成果 希望 各方 加强 协调 和 合作 把 特别 峰会 成果 落到实处 为 加强 抗疫 国际 合作 稳定 全球 经济 注入 强劲 动力 中方 愿同 包括 美方 在内 的 各方 一道 继续 支持 世卫 组织 发挥 重要 作用 加强 防控 信息 和 经验交流 共享 加快 科研 攻关 合作 推动 完善 全球 卫生 治理 加强 宏观经济 政策 协调 稳 市场 保 增长 保 民生 确保 全球 供应链 开放 稳定 安全 习近平 应询 详细 介绍 了 中方 为 打 好 疫情 防控 阻击战 采取 的 举措 习近平 强调 我 十分 关注 和 担心 美国 疫情 发展 也 注意 到 总统 先生 正在 采取 一系列 政策 举措 中国 人民 真诚 希望 美国 早日 控制 住 疫情 蔓延 势头 减少 疫情 给 美国 人民 带来 的 损失 中方 对 开展 国际 防控 合作 一向 持 积极态度 当前情况 下中美 应该 团结 抗疫 中美 两国 卫生部门 和 防控 专家 就 国际 疫情 形势 中 美 防控 合作 一直 保持 着 沟通 中方 愿 继续 毫无保留 同 美方 分享 信息 和 经验 中国 一些 省市 和 企业 纷纷 在 向 美方 提供 医疗 物资 援助 中方 理解 美方 当前 的 困难 处境 愿 提供 力所能及 的 支持 习近平 强调 目前 在 美国 仍 有 大量 中国 公民 包括 留学生 中国政府 高度重视 他们 的 生命安全 和 身体健康 希望 美方 采取 切实有效 措施 维护 好 他们 的 生命安全 和 身体健康 习近平 强调 当前 中美关系 正 处在 一个 重要 关口 中 美 合则两利 斗则 俱伤 合作 是 唯一 正确 的 选择 希望 美方 在 改善 中美关系 方面 采取 实质性 行动 双方 共同努力 加强 抗疫 等 领域 合作 发展 不 冲突 不 对抗 相互尊重 合作 共 赢 的 关系 特朗普 表示 我 认真 聆听 了 主席 先生 昨天晚上 在 二 十国集团 特别 峰会 上 的 讲话 我 和 各国 领导人 都 赞赏 你 提出 的 看法 和 倡议 特朗普 向 习近平 详细 询问 了 中方 有关 疫情 防控 举措 表示 美中 两国 都 正 面临 新冠 肺炎 疫情 挑战 我 高兴 看到 中方 在 抗击 疫情 方面 取得 了 积极 进展 中方 的 经验 对 我 很 有 启发 我 将 亲自 过问 确保 美中 两国 排除 干扰 集中精力 开展 抗疫 合作 感谢 中方 为 美方 抗疫 提供 医疗 物资供应 并 加强 两 国 医疗卫生 领域 交流 包括 抗疫 有效 药物 研发 方面 的 合作 我 在 社交 媒体 上 已 公开 表示 美国 人民 非常 尊敬 和 喜爱 中国 人民 中国 留学生 对 美国 教育 事业 非常 重要 美方 将 保护 好 在 美 中国 公民 包括 中国 留学生 两 国 元首 同意 就 共同 关心 的 问题 保持 沟通']\n",
      "['fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'real']\n",
      "tf-idf\n",
      "模型精准度:92.74\n",
      "bow\n",
      "模型精准度:88.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Manipulate_File(object):\n",
    "    def __init__(self):\n",
    "        self.file = \"../data/all_data.json\"\n",
    "        #  初始化各变量\n",
    "        self.txt = []\n",
    "        self.label = []\n",
    "        self.real_url = []\n",
    "        self.fake_url = []\n",
    "        '''\n",
    "        self.num1 = 0  # 开始读取文件的开始行号\n",
    "        self.num2 = 0  # 结束读取文件的行号\n",
    "        '''\n",
    "        self.counter_real = 0\n",
    "        self.counter_fake = 0\n",
    "\n",
    "        self.data = self.read_file()  # 用json解析文件\n",
    "        self.clean_and_add()  # 简化文件，并更新各变量\n",
    "\n",
    "\n",
    "        # self.test_size= self.best_parameter()\n",
    "        # self.stop_words = []\n",
    "\n",
    "    def read_file(self):  # 读取文件\n",
    "        with open(self.file, mode='r', encoding=\"UTF-8\") as data:\n",
    "            return json.load(data)\n",
    "\n",
    "    def clean_and_add(self):  # 将数据转化成词袋模型\n",
    "        for line in self.data:\n",
    "            clean_text = re.sub('\\W*', '', line['text'])  # 数据清理\n",
    "            token = jieba.cut(clean_text)  # 中⽂分词\n",
    "            final_token = \" \".join(token)\n",
    "            self.txt.append(final_token)\n",
    "            length = len(line['pic_url'])\n",
    "            self.label.append(line['label'])\n",
    "            if line['label'] == 'fake':\n",
    "                self.fake_url.append(length)\n",
    "                self.counter_fake += 1\n",
    "            else:\n",
    "                self.real_url.append(length)\n",
    "                self.counter_real += 1\n",
    "\n",
    "        \"\"\"\n",
    "        print(self.txt)\n",
    "        print(self.label)\n",
    "        \"\"\"\n",
    "\n",
    "    def pancake(self):  # 画出饼图\n",
    "        plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        y = ['real-data', 'fake-data']\n",
    "        x = [self.counter_real, self.counter_fake]\n",
    "        fig.set_size_inches(5, 5)\n",
    "        ax.pie(x, labels=y)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def bar(self):  # 画出直方图\n",
    "\n",
    "        fig1, ax1 = plt.subplots(1, 2, sharex=True, tight_layout=True)\n",
    "        fig1.set_size_inches(6, 6)\n",
    "        colors = ['red', 'green']\n",
    "        ax1[0].hist(self.real_url, color=colors[0])\n",
    "        ax1[1].hist(self.fake_url, color=colors[1])\n",
    "\n",
    "        ax1[0].set_title(\"real-data\")\n",
    "        ax1[0].set_xlabel('urls of real')\n",
    "        ax1[0].set_ylabel('amount of real')\n",
    "\n",
    "        ax1[1].set_title(\"fake-data\")\n",
    "        ax1[1].set_xlabel('urls of fake')\n",
    "        ax1[1].set_ylabel('amount of fake')\n",
    "        plt.show()\n",
    "\n",
    "    def best_parameter(self):  # 尝试找出最好的test_size，并将其返回\n",
    "        dic = {}\n",
    "\n",
    "        # np.arange(0.1, 1.0,0.1)建立一个从0.1到1.0，步长为0.1的列表\n",
    "        for decimal in np.arange(start=0.1, stop=1.0, step=0.1):\n",
    "            x_train, x_test, y_train, y_test = train_test_split(self.txt, self.label, test_size=decimal,\n",
    "                                                                random_state=2023)\n",
    "            pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                             ('model', LinearSVC())])\n",
    "            # 模型选择为LinearSVC\n",
    "            model = pipe.fit(x_train, y_train)\n",
    "            score = round(model.score(x_test, y_test) * 100, 2)\n",
    "            dic[decimal] = score\n",
    "        dic = sorted(dic.items(), key=lambda x: x[1], reverse=True)  # 经典集合排序\n",
    "        return dic[0][0]\n",
    "\n",
    "    def all_models(self):\n",
    "        models = [\"svm\", \"nb\", \"dt\", \"sgd\", \"rf\", \"gb\", \"kn\", \"lg\"]\n",
    "        for i in models:\n",
    "            x_train, x_test, y_train, y_test = train_test_split(\n",
    "                self.txt, self.label, test_size=.2, random_state=2023)\n",
    "            print(i)\n",
    "            if i == \"svm\":\n",
    "                pipe = Pipeline(\n",
    "                    [('vect', TfidfVectorizer()), ('model', LinearSVC())])\n",
    "            elif i == \"nb\":\n",
    "                pipe = Pipeline([('vect', TfidfVectorizer()),\n",
    "                                 ('model', MultinomialNB())])\n",
    "            elif i == \"dt\":\n",
    "                pipe = Pipeline([('vect', TfidfVectorizer()),\n",
    "                                 ('model', DecisionTreeClassifier())])\n",
    "            elif i == \"sgd\":\n",
    "                pipe = Pipeline([('vect', TfidfVectorizer()),\n",
    "                                 ('model', SGDClassifier())])\n",
    "            elif i == \"rf\":\n",
    "                pipe = Pipeline([('vect', TfidfVectorizer()),\n",
    "                                 ('model', RandomForestClassifier())])\n",
    "            elif i == \"gb\":\n",
    "                pipe = Pipeline([('vect', TfidfVectorizer()),\n",
    "                                 ('model', GradientBoostingClassifier())])\n",
    "            elif i == \"kn\":\n",
    "                pipe = Pipeline([('vect', TfidfVectorizer()),\n",
    "                                 ('model', KNeighborsClassifier())])\n",
    "            else:\n",
    "                pipe = Pipeline([('vect', TfidfVectorizer()),\n",
    "                                 ('model', LogisticRegression())])\n",
    "            model = pipe.fit(x_train, y_train)\n",
    "            score = round(model.score(x_test, y_test) * 100, 2)\n",
    "            print('模型精准度 : {}%'.format(score))\n",
    "\n",
    "\n",
    "\n",
    "    def TF_IDF(self):  # 两种模型拟合数据\n",
    "        new_txt = self.txt[40:60]\n",
    "        print(new_txt)\n",
    "        new_label = self.label[40:60]\n",
    "        print(new_label)\n",
    "\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        x = vectorizer.fit_transform(new_txt)\n",
    "        vectorizers = [\"tf-idf\", \"bow\"]\n",
    "        for i in vectorizers:\n",
    "            x_train, x_test, y_train, y_test = train_test_split(\n",
    "                self.txt, self.label, test_size=0.2, random_state=2023)\n",
    "            print(i)\n",
    "            if i == \"tf-idf\":\n",
    "                pipe = Pipeline(\n",
    "                    [('vect', TfidfVectorizer()), ('model', LinearSVC())])\n",
    "            else:\n",
    "                pipe = Pipeline(\n",
    "                    [('vect', CountVectorizer()), ('model', LinearSVC())])\n",
    "            model = pipe.fit(x_train, y_train)\n",
    "            score = round(model.score(x_test, y_test)*100, 2)\n",
    "            print('模型精准度:{}'.format(score))\n",
    "\n",
    "# 有待优化\n",
    "\n",
    "Manipulate_File().TF_IDF()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['美军 飞机 错误 地 把 中国 援助 日本 物资 偷运 到 美国 日本 日前 在 调运 中国 捐赠 的 2000 个 检测 试剂盒 期间 用 的 是 驻 日美军 的 运输机 结果 到达 目的地 之后 才 发现 这批 检测 试剂盒 已经 被 运到 美国 本土 美国共和党 议员 竟然 声称 检测 试剂盒 已经 用 完 了 现在 讨论 这个 问题 毫无意义 美国国会 大厦 参众两院 大楼 关闭', '襄阳 正式 封城 至此 整个 湖北 封省 封 一个 省护 一 国人 大家 多多保重 2 襄阳', '咱们 雷 神山 牛 防疫 有 效果 也 别 为 美国 瞎操心 了 美国 的 10 艘 医疗 军舰 已经 开进 纽约港 每艘 船上 有 1000 张 病床 上面 拥有 所以 的 医疗 抢救 设施 每艘 船 相当于 一个 雷 神山 明显 美国 对 远程 战争 早有 准备 没想到 第三次 世界大战 就 这样 开始 了 是 人类 对 病毒 的 战争 全国 确诊 新型 肺炎 病例 疫情 依旧 言殇', '武钢 武钢 董事长 邓琦琳 不是 应该 在 坐牢 吗 怎么 在 香港 到处 传播 病毒 确诊 肺炎 这是 谣言 还是 被 肺炎 抓住 的 贪官 2 青岛', '我 不是 中国 人 华人 女子 获 美国 绿卡 慷慨 对美 捐赠 20 万 只 口罩 我 已经 获得 美国 绿卡 即将 加入 美国 国籍 现在 已经 不是 中国 人 短短的 一句 话 在 海外 社交 网站 引起 了 巨大 轰动 据 海外 媒体 整理 这段话 是 一名 来自 中国宁波 的 女子 为了 快速 加入 美国 国籍 该 女子 以 个人 名义 在 中国 采购 了 20 万 只 口罩 捐赠 给 美国 国土 安全部 抵抗 疫情 为了 获得 美国 绿卡 加入 美国 国籍 这名 女子 除了 捐赠 口罩 之外 还 从 日本 采购 了 3 台 医疗 设备 献给 了 美国陆军 野战医院 用于 疑似 感染 的 美军 士兵 治疗 保守 估计 该 女子 现如今 已经 给 美国 捐赠 了 超过 300 万美元 的 物资 而 这 一切 仅仅 是 为了 加速 获得 美国 国籍 来源 兵戎 要志', '西安 爆料 逃离 武汉 大 作战 老 户县 新 鄠 hu 邑 yi 蒋村 小伙 吴 建章 此人 在 武汉 感染 后 逃离 回家 被 发现 后 在 户县 北关 医院 隔离 治疗 晚上 8 点 左右 又 一次 逃离 大家 年 都 过 不好 了 一只 老鼠 坏 了 一 锅汤 一声 叹息 一地 鸡毛 产经 智库 呼吁 希望 大家 转发 看到 此人 迅速 报警 别 让 病毒 再次 扩散', '最 硬 管控 网友 爆料 湖北 武穴市 凡 不 属于 疫情 防控 人员 上街 被 抓 的 一律 送到 市 体育馆 会展中心 学习 还要 完成 初三 黄冈 密卷 一张 成绩及格 者 可以 回家 怕 是 有人 14 天 也 回 不了 家 了 感受 到 了 来自 学校 的 关爱', '武汉 三民 小区 的 人 全变 红码 全市 所有人 全部 核酸 检测 武汉 三民 小区 出 了 六个 感染者 清零 很 久 以后 出 的 震动 全国 是 一个 89 岁 的 之前 感染 自己 好了没 去 治疗 隔 了 两个 月 又 发病 传染 人 武汉 发了 狠 所有人 除 六岁 以下 全部 强制 检测 三民 小区 封掉 5000 人 都 变红 码动 不了 又 隔离 14 天 上班 的 也 不行 了 全市 10 天都测 一遍 不测 的 人变 黄码 无法 出行 过期 检测 自己 出钱 要测 一个 小区 就 提前 封闭 这 是 搞 毛 了 被 病毒 害 的 太 惨大 样本 检测 科研 意义 也 很大', '张文宏 医生 自爆 收入 和 全家福 69 年 生人 浙江 瑞安 人 从小 喜欢 读书 学习 成绩优秀 在 老家 被 邻里 称为 神童 高考 考入 上海医学院 8 年 硕博 连读 后 留学 现是 全球 知名 的 传染病 专家 自爆 年收入 184 万 来源于 3 个 方面 1 华山医院 呼吸科 主任 工资 奖金 年入 50 万 2 国家 重点项目 学科 带头人 年入 120 万 3 每年 在 全球 顶尖 医学杂志 柳叶刀 上 发表 一篇 署名 论文 专利费 2 万美元 首个 公布 财产 的 医生 一个 干净 的 人强 在 看看 院士 高福 号称 在 柳叶刀 上 发表 过 500 多篇 论文 是不是 也 应该 公布 一下 自己 的 收入 呢 张文宏 大 浙江 2 北京 北京 天坛 饭店', '美国 的 10 艘 医疗 军舰 已经 开进 纽约港 每艘 船上 有 1000 张 病床 上面 拥有 所有 的 医疗 抢救 设施 每艘 船 相当于 一个 雷 神山 医院 如果 不是 这次 病毒 我们 都 不 知道 美国 的 医疗 底牌 呢 海外 疫情 特朗普 称 美国 经济 可能 走向 衰退', '我 不是 中国 人 华人 女子 获 美国 绿卡 慷慨 对美 捐赠 20 万 只 口罩 我 已经 获得 美国 绿卡 即将 加入 美国 国籍 现在 已经 不是 中国 人 短短的 一句 话 在 海外 社交 网站 引起 了 巨大 轰动 据 海外 媒体 整理 这段话 是 一名 来自 中国宁波 的 女子 为了 快速 加入 美国 国籍 该 女子 以 个人 名义 在 中国 采购 了 20 万 只 口罩 捐赠 给 美国 国土 安全部 抵抗 疫情 为了 获得 美国 绿卡 加入 美国 国籍 这名 女子 除了 捐赠 口罩 之外 还 从 日本 采购 了 3 台 医疗 设备 献给 了 美国陆军 野战医院 用于 疑似 感染 的 美军 士兵 治疗 保守 估计 该 女子 现如今 已经 给 美国 捐赠 了 超过 300 万美元 的 物资 而 这 一切 仅仅 是 为了 加速 获得 美国 国籍 来源 兵戎 要志', '我 不是 中国 人 华人 女子 获 美国 绿卡 慷慨 对美 捐赠 20 万 只 口罩 此 视频 已有 11 万次 播放 快 来 一起 看看 O 网页 链接', '祸不单行 美国 确诊 123 万 刚刚 凌晨 化工厂 大 爆炸 近 6 万人 受灾 上帝 也 要 惩罚 美利坚 了 吗 今日 热 搜 头条 美国 国内 疫情 严重 够 特朗普 喝 一壶 的 还有 心思 到处 惹是生非 甩锅 侮辱 中国 在 中东 波斯湾 搞 事情 威胁 伊朗 粗暴 干扰 他 国内 政人 在 做 天 在 看 美国 得克萨斯州 凌晨 一点 一座 化工厂 发生 严重 爆炸 火光 四起 巨大 火光 震动 波及 到 数公里 之外 的 居民 到 目前为止 已知 10 人 被 玻璃 划伤 数名 工人 伤亡 只 隔 几小时 又 发生 第二 波 爆炸 引起 惊恐万分 54 万 居民 连夜 紧急 撤退 到 现在 为止 究竟 是 天灾 还是 人祸 不得而知 但是 爆炸 释放出来 的 有毒气体 已经 袭击 无数 市民 似乎 是 上帝 在 提醒 美国 不要 搞 事情 挑起 战乱 制造事端 先 管理 好 自己 千疮百孔 的 国家 吐槽', '香港 失业率 52 创 十年 新高 香港特区政府 统计 处 19 日 公布 2 月 至 4 月经 季节性 调整 的 失业率 升至 52 创 逾 10 年 新高 就业 不足 率为 31 是 超过 15 年 来 的 高位 特区政府 劳工 及 福利 局局长 罗致 光 表示 新冠 疫情 继续 广泛 影响 经济 活动 劳工市场 急剧 恶化 2 月 至 4 月 的 总 就业人数 和 劳动 人口 均 创 历史 最大 按年 跌幅 零售 住宿 及 餐饮 服务 等 相关 行业 的 合计 失业率 急 升至 9 创 逾 15 年 新高 就业 不足 率 也 急 升至 59 是 有 纪录 以来 的 最高 位 人民日报 记者 陈然', '美国 新冠 肺炎 超 221 万 美国 日 新增 确诊 超 3 万例 据 美国 约翰斯 霍普金斯大学 疫情 实时 监测 系统 显示 截至 美 东 时间 6 月 19 日 下午 5 时 33 分 美国 已有 新冠 病毒感染 病例 2215587 例 其中 包括 死亡 病例 118991 例 与 该 系统 约 24 小时 前 的 数据 相比 美国 新增 感染 病例 33302 例 新增 死亡 病例 695 例 人民日报 记者 郑琪', '钟南山 称 不 从 全球 范围 内 控制 好 不 可能 战胜 疫情 3 月 12 日 广东省 人民政府 新闻 办公室 举行 新闻 发布会 钟南山 称 6 月份 结束 疫情 是 可以 期待 的 但是 也 要 取决于 各个 国家 的 重视 程度 要 在 两个 月 内 研发 出 特效药 或者 特殊 的 做法 还是 不太可能 因此 现在 一方面 要 加强 自身 管控 另一方面 要 加强 与 国外 的 交流 现在 不 从 全球 范围 内 控制 好 的话 是 不 可能 战胜 疫情', '日本 全国 紧急状态 将 延长 据 日本广播协会 NHK 电视台 统计 截至 4 日 10 时 30 分 北京 时间 9 时 30 分 日本 24 小时 内 新增 新冠 确诊 病例 202 例 累计 确诊 15079 例 新增 死亡 病例 19 例 累计 死亡 病例 536 例 日本首相 安倍晋三 将 于 4 日 傍晚 正式 宣布 延长 紧急状态 4 日 上午 日本政府 召集 了 专家 小组 举行 会议 讨论 疫情 长期化 状态 下 的 防疫 和 经济 活动 对策 之后 又 召开 了 咨询 委员会 会议 围绕 将 全国 紧急状态 延长 到 5 月 31 日 的 方针 向 咨询 委员会 专家 进行 咨询 新华社', '千里 为 邻战疫 必胜 湖北 捐助 黑龙江 首批 医用 物资 启程 赴 绥芬河 15 日 1106 湖北 向 黑龙江 捐助 的 第一批 医用 物资 从 武汉 天河 国际 机场 启程 飞往 黑龙江省 绥芬河市 这批 物资 包括 医用 防护服 5 万套 N95 口罩 10 万 只 医用 外科 口罩 100 万 只 一次性 医用 口罩 200 万 只 移动 DR1 台 正压 呼吸器 50 套及 配件 2500 套 空气 消毒机 300 套 有 创 呼吸机 5 套 心电监护 仪 100 台 注射 泵 121 台 输液泵 63 台 据介绍 自 1 月 27 日起 黑龙江省 共 派出 8 批 共计 1554 名 医护 工作者 援助 武汉市 和 孝感市 黑龙江 省委 省政府 还 紧急 调集 3000 吨 大米 等 物资 有力 保障 了 湖北 疫情 防控 物资 需求 王 翔宇 李伟张 小舟', '继续 加油 北京 连续 4 天零 新增 北京 中 风险 地区 15 个 7 月 9 日 0 时至 24 时 北京 无 新增 报告 本地 确诊 病例 疑似病例 和 无症状 感染者 治愈 出院 病例 12 例无 新增 报告 境外 输入 确诊 病例 疑似病例 和 无症状 感染者 6 月 11 日 0 时至 7 月 9 日 24 时 累计 报告 本地 确诊 病例 335 例在院 263 例 治愈 出院 72 例尚 在 观察 的 无症状 感染者 24 例无 新增 报告 境外 输入 新冠 肺炎 确诊 病例 疑似病例 和 无症状 感染者 7 月 8 日 0 时至 24 时 丰台区 南苑 街道 南苑 地区 乡 大兴区 观音寺 街道 由 中 风险 地区 调整 为 低 风险 地区 截至 7 月 8 日 24 时 我市 共有 高风险 地区 1 个 为 丰台区 花乡 地区 乡 共有 中 风险 地区 15 个 为 海淀区 田村路 街道 四季青 地区 镇 丰台区 丰台 街道 卢沟桥 街道 马家堡 街道 卢沟桥 地区 乡 新村 街道 大兴区 北 臧 村镇 黄村 地区 镇 青云 店镇 魏善 庄镇 兴丰 街道 高 米店 街道 西红门 地区 镇 昌平区 回龙观 街道', '习近平 同 美国 总统 特朗普 通电话 国家 主席 习近平 27 日 应约 同 美国 总统 特朗普 通电话 习近平 强调 新冠 肺炎 疫情 发生 以来 中方 始终 本着 公开 透明 负责 任 态度 及时 向世卫 组织 以及 包括 美国 在内 的 有关 国家 通报 疫情 信息 包括 第一 时间 发布 病毒基因 序列 等 信息 毫无保留 地同 各方 分享 防控 和 治疗 经验 并 尽己 所能 为 有 需要 的 国家 提供 支持 和 援助 我们 将 继续 这样 做同 国际 社会 一道 战胜 这场 疫情 习近平 指出 流行性 疾病 不 分 国界 和 种族 是 人类 共同 的 敌人 国际 社会 只有 共同 应对 才能 战而胜 之 在 各方 共同努力 下 昨天 举行 的 二 十国集团 领导人 应对 新冠 肺炎 特别 峰会 达成 不少 共识 取得 积极 成果 希望 各方 加强 协调 和 合作 把 特别 峰会 成果 落到实处 为 加强 抗疫 国际 合作 稳定 全球 经济 注入 强劲 动力 中方 愿同 包括 美方 在内 的 各方 一道 继续 支持 世卫 组织 发挥 重要 作用 加强 防控 信息 和 经验交流 共享 加快 科研 攻关 合作 推动 完善 全球 卫生 治理 加强 宏观经济 政策 协调 稳 市场 保 增长 保 民生 确保 全球 供应链 开放 稳定 安全 习近平 应询 详细 介绍 了 中方 为 打 好 疫情 防控 阻击战 采取 的 举措 习近平 强调 我 十分 关注 和 担心 美国 疫情 发展 也 注意 到 总统 先生 正在 采取 一系列 政策 举措 中国 人民 真诚 希望 美国 早日 控制 住 疫情 蔓延 势头 减少 疫情 给 美国 人民 带来 的 损失 中方 对 开展 国际 防控 合作 一向 持 积极态度 当前情况 下中美 应该 团结 抗疫 中美 两国 卫生部门 和 防控 专家 就 国际 疫情 形势 中 美 防控 合作 一直 保持 着 沟通 中方 愿 继续 毫无保留 同 美方 分享 信息 和 经验 中国 一些 省市 和 企业 纷纷 在 向 美方 提供 医疗 物资 援助 中方 理解 美方 当前 的 困难 处境 愿 提供 力所能及 的 支持 习近平 强调 目前 在 美国 仍 有 大量 中国 公民 包括 留学生 中国政府 高度重视 他们 的 生命安全 和 身体健康 希望 美方 采取 切实有效 措施 维护 好 他们 的 生命安全 和 身体健康 习近平 强调 当前 中美关系 正 处在 一个 重要 关口 中 美 合则两利 斗则 俱伤 合作 是 唯一 正确 的 选择 希望 美方 在 改善 中美关系 方面 采取 实质性 行动 双方 共同努力 加强 抗疫 等 领域 合作 发展 不 冲突 不 对抗 相互尊重 合作 共 赢 的 关系 特朗普 表示 我 认真 聆听 了 主席 先生 昨天晚上 在 二 十国集团 特别 峰会 上 的 讲话 我 和 各国 领导人 都 赞赏 你 提出 的 看法 和 倡议 特朗普 向 习近平 详细 询问 了 中方 有关 疫情 防控 举措 表示 美中 两国 都 正 面临 新冠 肺炎 疫情 挑战 我 高兴 看到 中方 在 抗击 疫情 方面 取得 了 积极 进展 中方 的 经验 对 我 很 有 启发 我 将 亲自 过问 确保 美中 两国 排除 干扰 集中精力 开展 抗疫 合作 感谢 中方 为 美方 抗疫 提供 医疗 物资供应 并 加强 两 国 医疗卫生 领域 交流 包括 抗疫 有效 药物 研发 方面 的 合作 我 在 社交 媒体 上 已 公开 表示 美国 人民 非常 尊敬 和 喜爱 中国 人民 中国 留学生 对 美国 教育 事业 非常 重要 美方 将 保护 好 在 美 中国 公民 包括 中国 留学生 两 国 元首 同意 就 共同 关心 的 问题 保持 沟通']\n",
      "['fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'fake', 'real', 'real', 'real', 'real', 'real', 'real', 'real']\n",
      "tf-idf\n",
      "模型精准度:92.74\n",
      "bow\n",
      "模型精准度:88.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Manipulate_File(object):\n",
    "    def __init__(self):\n",
    "        self.file = \"../data/all_data.json\"\n",
    "        #  初始化各变量\n",
    "        self.txt = []\n",
    "        self.label = []\n",
    "        self.real_url = []\n",
    "        self.fake_url = []\n",
    "        '''\n",
    "        self.num1 = 0  # 开始读取文件的开始行号\n",
    "        self.num2 = 0  # 结束读取文件的行号\n",
    "        '''\n",
    "        self.counter_real = 0\n",
    "        self.counter_fake = 0\n",
    "\n",
    "        self.data = self.read_file()  # 用json解析文件\n",
    "        self.clean_and_add()  # 简化文件，并更新各变量\n",
    "\n",
    "\n",
    "        # self.test_size= self.best_parameter()\n",
    "        # self.stop_words = []\n",
    "\n",
    "    def read_file(self):  # 读取文件\n",
    "        with open(self.file, mode='r', encoding=\"UTF-8\") as data:\n",
    "            return json.load(data)\n",
    "\n",
    "    def clean_and_add(self):  # 将数据转化成词袋模型\n",
    "        for line in self.data:\n",
    "            clean_text = re.sub('\\W*', '', line['text'])  # 数据清理\n",
    "            token = jieba.cut(clean_text)  # 中⽂分词\n",
    "            final_token = \" \".join(token)\n",
    "            self.txt.append(final_token)\n",
    "            length = len(line['pic_url'])\n",
    "            self.label.append(line['label'])\n",
    "            if line['label'] == 'fake':\n",
    "                self.fake_url.append(length)\n",
    "                self.counter_fake += 1\n",
    "            else:\n",
    "                self.real_url.append(length)\n",
    "                self.counter_real += 1\n",
    "\n",
    "        \"\"\"\n",
    "        print(self.txt)\n",
    "        print(self.label)\n",
    "        \"\"\"\n",
    "\n",
    "    def pancake(self):  # 画出饼图\n",
    "        plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        y = ['real-data', 'fake-data']\n",
    "        x = [self.counter_real, self.counter_fake]\n",
    "        fig.set_size_inches(5, 5)\n",
    "        ax.pie(x, labels=y)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def bar(self):  # 画出直方图\n",
    "\n",
    "        fig1, ax1 = plt.subplots(1, 2, sharex=True, tight_layout=True)\n",
    "        fig1.set_size_inches(6, 6)\n",
    "        colors = ['red', 'green']\n",
    "        ax1[0].hist(self.real_url, color=colors[0])\n",
    "        ax1[1].hist(self.fake_url, color=colors[1])\n",
    "\n",
    "        ax1[0].set_title(\"real-data\")\n",
    "        ax1[0].set_xlabel('urls of real')\n",
    "        ax1[0].set_ylabel('amount of real')\n",
    "\n",
    "        ax1[1].set_title(\"fake-data\")\n",
    "        ax1[1].set_xlabel('urls of fake')\n",
    "        ax1[1].set_ylabel('amount of fake')\n",
    "        plt.show()\n",
    "\n",
    "    def best_parameter(self):  # 尝试找出最好的test_size，并将其返回\n",
    "        dic = {}\n",
    "\n",
    "        # np.arange(0.1, 1.0,0.1)建立一个从0.1到1.0，步长为0.1的列表\n",
    "        for decimal in np.arange(start=0.1, stop=1.0, step=0.1):\n",
    "            x_train, x_test, y_train, y_test = train_test_split(self.txt, self.label, test_size=decimal,\n",
    "                                                                random_state=2023)\n",
    "            pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                             ('model', LinearSVC())])\n",
    "            # 模型选择为LinearSVC\n",
    "            model = pipe.fit(x_train, y_train)\n",
    "            score = round(model.score(x_test, y_test) * 100, 2)\n",
    "            dic[decimal] = score\n",
    "        dic = sorted(dic.items(), key=lambda x: x[1], reverse=True)  # 经典集合排序\n",
    "        return dic[0][0]\n",
    "\n",
    "    def all_models(self):\n",
    "        models = [\"svm\", \"nb\", \"dt\", \"sgd\", \"rf\", \"gb\", \"kn\", \"lg\"]\n",
    "        for i in models:\n",
    "            x_train, x_test, y_train, y_test = train_test_split(\n",
    "                self.txt, self.label, test_size=.2, random_state=2023)\n",
    "            print(i)\n",
    "            if i == \"svm\":\n",
    "                pipe = Pipeline(\n",
    "                    [('vect', TfidfVectorizer()), ('model', LinearSVC())])\n",
    "            elif i == \"nb\":\n",
    "                pipe = Pipeline([('vect', TfidfVectorizer()),\n",
    "                                 ('model', MultinomialNB())])\n",
    "            elif i == \"dt\":\n",
    "                pipe = Pipeline([('vect', TfidfVectorizer()),\n",
    "                                 ('model', DecisionTreeClassifier())])\n",
    "            elif i == \"sgd\":\n",
    "                pipe = Pipeline([('vect', TfidfVectorizer()),\n",
    "                                 ('model', SGDClassifier())])\n",
    "            elif i == \"rf\":\n",
    "                pipe = Pipeline([('vect', TfidfVectorizer()),\n",
    "                                 ('model', RandomForestClassifier())])\n",
    "            elif i == \"gb\":\n",
    "                pipe = Pipeline([('vect', TfidfVectorizer()),\n",
    "                                 ('model', GradientBoostingClassifier())])\n",
    "            elif i == \"kn\":\n",
    "                pipe = Pipeline([('vect', TfidfVectorizer()),\n",
    "                                 ('model', KNeighborsClassifier())])\n",
    "            else:\n",
    "                pipe = Pipeline([('vect', TfidfVectorizer()),\n",
    "                                 ('model', LogisticRegression())])\n",
    "            model = pipe.fit(x_train, y_train)\n",
    "            score = round(model.score(x_test, y_test) * 100, 2)\n",
    "            print('模型精准度 : {}%'.format(score))\n",
    "\n",
    "\n",
    "\n",
    "    def TF_IDF(self):  # 两种模型拟合数据\n",
    "        new_txt = self.txt[40:60]\n",
    "        print(new_txt)\n",
    "        new_label = self.label[40:60]\n",
    "        print(new_label)\n",
    "\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        x = vectorizer.fit_transform(new_txt)\n",
    "        vectorizers = [\"tf-idf\", \"bow\"]\n",
    "        for i in vectorizers:\n",
    "            x_train, x_test, y_train, y_test = train_test_split(\n",
    "                self.txt, self.label, test_size=0.2, random_state=2023)\n",
    "            print(i)\n",
    "            if i == \"tf-idf\":\n",
    "                pipe = Pipeline(\n",
    "                    [('vect', TfidfVectorizer()), ('model', LinearSVC())])\n",
    "            else:\n",
    "                pipe = Pipeline(\n",
    "                    [('vect', CountVectorizer()), ('model', LinearSVC())])\n",
    "            model = pipe.fit(x_train, y_train)\n",
    "            score = round(model.score(x_test, y_test)*100, 2)\n",
    "            print('模型精准度:{}'.format(score))\n",
    "\n",
    "# 有待优化\n",
    "\n",
    "Manipulate_File().TF_IDF()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2b02b1cc90ba81b07904beb77a8b712ee9d9a0747d5159d5688cdbae478a112"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env_1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}